# 第 6 部分：智能体中的规划与推理模型


---

## 哇！我们的课程已经过半了！

在过去的几个部分中，我们讨论了智能体可以做什么：
- 使用工具
- 通过 RAG 检索信息
- 使用 MCP 以清晰的格式传递所有内容

但所有这些都基于一个基本假设：
**智能体实际上知道下一步该做什么。**
而这往往是事情出错的地方。

今天，我们将重点从工具和输入转移到**智能体如何思考**——更具体地说，现代模型如何开始规划，以及为什么这改变了我们设计现实世界系统的方式。

---

## 为什么规划在 Agentic 系统中很重要

让我们从几个例子开始。

如果你问一个智能体：
> “13 乘以 47 是多少？”
……它可以直接解决，也可以调用计算器。这是一个单步骤任务——不需要真正的规划。

现在想象一下问：
> “找到我们第一季度所有医疗保健行业的客户，检查哪些客户逾期未付款，并起草带有新付款链接的个性化电子邮件。”

在这种情况下，智能体需要：
- 理解指令
- 将其分解为可管理的部分
- 检索正确的数据
- 选择工具
- 按顺序执行步骤
- 处理异常
- 知道任务何时完成

这种解释、排序和行动的循环就是**规划**。

智能体（指模型）需要自己解决这些问题——包括使用哪些工具以及如何应用它所拥有的信息。

---

## 为什么传统 LLM 在规划方面存在困难

大多数通用 LLM 从未接受过这方面的训练。

它们只是根据之前的上下文**预测下一个 token**——仅此而已。
它们擅长：
- 续写句子
- 生成摘要
- 回答直接问题

……但它们更像是**短视的生成器**。
它们完成眼前的任务，但并没有提前思考的能力。

当被要求在多步骤、决策型任务中充当智能体时，它们往往会：
- 跳过步骤
- 重复操作
- 把简单的事情复杂化
- 中途迷失方向

---

## 早期改进推理的尝试

为了弥补这一差距，开发者尝试使用提示技术来引导规划行为。

一个流行的例子是：**思维链提示**——添加“让我们一步一步思考”来将任务分解为阶段。

这在逻辑谜题和结构化问答中有效，但对于**与工具、不可预测的输入和不断变化的状态一起工作的真实智能体**来说，效果不佳。

因为从根本上说，这些模型仍然没有接受过规划训练——它们只是对**提示技巧**做出反应。

---

## 然后推理模型出现了

下一个转变：训练模型**从设计上进行规划**。

这催生了**大型推理模型（LRMs）**。
<img width="743" height="663" alt="image" src="https://github.com/user-attachments/assets/ce4d8d91-b539-4003-adfc-1fa6dcfd3631" />

**LLMs：**
输入 → LLM → 输出语句

**LRMs：**
输入 → LRM → 规划步骤 + 输出语句


所有仍然是文本，但 LRMs 在训练过程中被引导**先思考再行动**。

---

**例子：**
- OpenAI 的 **o 系列**（o1, o3）——第一个公开示例
- DeepSeek 的 **DeepSeek-R1**——针对工具增强的推理和规划进行了优化
- Google 的 **Gemini 思维模型**
- Anthropic 的 **Claude 3.7 推理模式**

有些甚至只在**需要时才激活推理**。

---

## 它们如何融入 Agentic 设计

推理模型的主要价值在于改进**规划组件**——即提出“我下一步应该做什么，为什么？”的部分。

在企业用例中，**规划是智能体经常失败的地方**。
推理模型可以提供帮助，但它们并非魔法。

---

## 谨慎使用

推理模型仍然**很新**，并且存在权衡：
- 对简单任务过度思考
- 生成更长的输出
- 增加延迟和成本
- 可能产生听起来合理但不正确的规划

**经验法则：**
- 不要从推理模型开始。
- 从一个中等规模的基础模型开始。
- 只有当你看到明显的规划失败时才切换——即使如此，也要评估实际影响。

---

## 接下来

在下一部分中，我们将转向智能体的另一个**核心组件**：**记忆**——智能体如何有效记忆以及为什么这很重要。